---
title: "Replication of Boutonnet & Lupyan"
output: html_notebook
---

```{r message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(lme4)
library(lmerTest)
library(tidyr)
```

# Behavioral Data

Load in the behavioral data and convert columns to correct types as needed.

```{r message=FALSE, warning=FALSE}
behavioral.data <- read_csv('data/behavioral/generated/behavioral_data.csv')
behavioral.data$participant_id <- factor(behavioral.data$participant_id)
behavioral.data$correct <- as.logical(behavioral.data$correct)
behavioral.data$rt <- as.numeric(behavioral.data$rt)
```

## Accuracy

Describe mean accuracy in each condition

```{r}
accuracy.data <- behavioral.data %>% 
  filter(phase=="test") %>%
  group_by(participant_id, match_type, audio_type) %>%
  summarize(accuracy=mean(correct)*100) %>%
  group_by(match_type, audio_type) %>%
  summarize(M=mean(accuracy), SD = sd(accuracy))
accuracy.data
```

## Figure 1

Recreate Figure 1 from Boutonnet & Lupyan.

```{r}
plot.data <- behavioral.data %>% 
  filter(phase=="test") %>%
  filter(correct==TRUE) %>%
  group_by(participant_id, match_type, audio_type) %>%
  summarize(mean.rt=mean(as.numeric(rt))) %>%
  group_by(match_type, audio_type) %>%
  summarize(M=mean(mean.rt), SE = sd(mean.rt) / sqrt(n()))
                                                                  
ggplot(plot.data, aes(x=match_type,y=M,ymin=M-SE, ymax=M+SE,color=audio_type))+
  geom_pointrange(position=position_dodge(width=0.5), size=0.75)+
  scale_color_brewer(type="qual",palette="Set1", labels=c("Label","Sound"))+
  scale_x_discrete(labels=c("Match", "Mismatch"))+
  labs(x="Congruency",y="Reaction Time (ms)", color="Cue Type")+
  theme_bw()+
  theme(panel.grid.major.x=element_blank())
```

## Mixed effects model for behavioral data

Need data to be every trial

```{r}
beh.model.data <- behavioral.data %>% 
  filter(phase=="test") %>%
  filter(correct==TRUE) %>%
  select(participant_id, audio_type, match_type, rt) %>%
  mutate(match_type = factor(match_type), audio_type = factor(audio_type))

# simple.model <- lmer(rt ~ match_type*audio_type + (1 | participant_id), data=beh.model.data)
# summary(simple.model)

beh.model <- lmer(rt ~ match_type*audio_type + (audio_type | participant_id) + (match_type | participant_id), data=beh.model.data, control=lmerControl(optimizer="bobyqa"))
summary(beh.model)
```


# EEG Data

```{r message=FALSE, warning=FALSE}
averaged.eeg.data <- read_csv('data/eeg/generated/all_averaged.csv')
```

## P1, P2: label vs. sound

```{r}

ga.eeg.p1 <- averaged.eeg.data %>% 
  filter(location == "occipital") %>% 
  group_by(subject, hemisphere, audio, t) %>%
  summarize(voltage = mean(voltage)) %>%
  group_by(hemisphere, audio, t) %>%
  summarize(m.voltage = mean(voltage), se = sd(voltage)/sqrt(n()))

ggplot(ga.eeg.p1, aes(x=t, y=m.voltage, ymin=m.voltage-se, ymax=m.voltage+se, color=audio, fill=audio))+
  geom_hline(yintercept = 0, color="black")+
  geom_vline(xintercept = 0, color="black")+
  geom_ribbon(alpha=0.2, color=NA)+
  geom_line()+
  scale_x_continuous(limits=c(-100,300))+
  facet_wrap(~hemisphere)+
  theme_bw()

```

## P2: match vs. non-match

```{r}
ga.eeg.p2 <- averaged.eeg.data %>% 
  filter(location=="occipital") %>%
  group_by(subject, hemisphere, congruence, t) %>%
  summarize(voltage = mean(voltage)) %>%
  group_by(hemisphere, congruence, t) %>%
  summarize(m.voltage = mean(voltage), se = sd(voltage)/sqrt(n()))

ggplot(ga.eeg.p2, aes(x=t, y=m.voltage, ymin=m.voltage-se, ymax=m.voltage+se, color=congruence, fill=congruence))+
  geom_hline(yintercept = 0, color="black")+
  geom_vline(xintercept = 0, color="black")+
  geom_ribbon(alpha=0.2, color=NA)+
  geom_line()+
  scale_x_continuous(limits=c(-100,300))+
  facet_wrap(~hemisphere)+
  theme_bw()

```

## N400

First plotting grand averages...

```{r echo=TRUE}
ga.eeg.n4 <- averaged.eeg.data %>% 
  filter(location == "parietal") %>% 
  group_by(subject, hemisphere, congruence, audio, t) %>%
  summarize(voltage = mean(voltage)) %>%
  group_by(hemisphere, congruence, audio, t) %>%
  summarize(m.voltage = mean(voltage), se = sd(voltage)/sqrt(n()))

ggplot(ga.eeg.n4, aes(x=t, y=m.voltage, ymin=m.voltage-se, ymax=m.voltage+se, linetype=audio, color=congruence, fill=congruence))+
  geom_hline(yintercept = 0, color="black")+
  geom_vline(xintercept = 0, color="black")+
  geom_ribbon(alpha=0.2, color=NA)+
  geom_line()+
  scale_x_continuous(limits=c(-100,600))+
  theme_bw()

```

Then plotting difference waves for each audio condition ...

```{r echo=TRUE}
n4.diff.data <- averaged.eeg.data %>% 
  filter(location == "parietal") %>% 
  group_by(subject, hemisphere, congruence, audio, t) %>%
  summarize(voltage = mean(voltage)) %>%
  ungroup() %>%
  pivot_wider(names_from = congruence, values_from=voltage) %>%
  mutate(voltage.diff =`non-match` - match) %>%
  group_by(hemisphere, audio,  t) %>%
  summarize(m.voltage = mean(voltage.diff), se = sd(voltage.diff)/sqrt(n()))

ggplot(n4.diff.data, aes(x=t, y=m.voltage, ymin=m.voltage-se, ymax=m.voltage+se, color=audio, fill=audio))+
  geom_hline(yintercept = 0, color="black")+
  geom_vline(xintercept = 0, color="black")+
  geom_ribbon(alpha=0.2, color=NA)+
  geom_line()+
  scale_x_continuous(limits=c(-100,600))+
  theme_bw()
```
